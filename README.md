# Clinical Trials Classification System

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)
![Transformers](https://img.shields.io/badge/Transformers-4.12+-green.svg)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)

This repository contains code for an NLP-based classification system that categorizes clinical trial descriptions into different medical conditions. The system leverages both traditional ML approaches and state-of-the-art transformer models to achieve high accuracy in categorizing medical texts.

## ðŸ“‹ Table of Contents
- [Problem Statement](#problem-statement)
- [Dataset](#dataset)
- [Methodology](#methodology)
- [Results](#results)
- [Model Deployment](#model-deployment)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Google Colab Deployment](#google-colab-deployment)
- [Contributors](#contributors)
- [References](#references)

---

## Problem Statement

Clinical trial descriptions contain rich medical information but are typically unstructured. Automatically classifying these descriptions into specific medical conditions can facilitate:
- Improved searchability of clinical trial databases
- Enhanced patient-trial matching
- Better understanding of research trends across different conditions

This project aims to develop a robust classifier that categorizes clinical trial descriptions into five medical conditions: **ALS**, **Dementia**, **Obsessive Compulsive Disorder**, **Parkinson's Disease**, and **Scoliosis**.

---

## Dataset

The dataset consists of **1,759 clinical trial descriptions** with the following distribution:
- ALS: 368 trials (20.9%)
- Dementia: 368 trials (20.9%)
- Obsessive Compulsive Disorder: 358 trials (20.4%)
- Scoliosis: 335 trials (19.0%)
- Parkinson's Disease: 330 trials (18.8%)

**Dataset structure**:
- Shape: (1759, 3)
- Columns: nctid (object), description (object), label (object)
- No missing values in any column

**Description length statistics**:
- Average (mean) length: **2,016.46 characters**
- Standard deviation: **2,103.19 characters**
- Minimum length: **9 characters**
- 25th percentile: **747 characters**
- Median length: **1,406 characters** 
- 75th percentile: **2,429.5 characters**
- Maximum length: **21,765 characters**

**Word count statistics**:
- Average (mean) count: **296.64 words**
- Standard deviation: **310.79 words**
- Minimum count: **2 words**
- 25th percentile: **112 words**
- Median count: **207 words**
- 75th percentile: **355 words**
- Maximum count: **3,297 words**

The dataset is well-balanced across all five medical conditions, which is beneficial for building a robust classifier. The variance in text length suggests the need for effective text normalization techniques.

---

## Methodology

### 1. Data Exploration and Preprocessing

- **Text cleaning**: Removal of special characters, digits, and punctuation
- **Normalization**: Text lowercasing, lemmatization
- **Stopword removal**: Both standard English stopwords and domain-specific medical stopwords
- **Vocabulary analysis**: Distinctive term identification for each medical condition

### 2. Feature Engineering

Multiple feature extraction methods were evaluated:
- Binary Occurrence (Bag of Words)
- Term Frequency (TF)
- TF-IDF (Term Frequency-Inverse Document Frequency)
- IDF Only (Inverse Document Frequency)

For transformer-based models, the raw text was tokenized using model-specific tokenizers.

### 3. Model Development

We explored two categories of models:

#### Traditional ML Models:
- **Logistic Regression** with L1 and L2 regularization
- **Support Vector Machines** (Linear SVC)
- **Random Forest**
- **Multinomial Naive Bayes**

#### Transformer-Based Models:
- **BERT** (bert-base-uncased)
- **BioBERT** (dmis-lab/biobert-v1.1)
- **ClinicalBERT** (emilyalsentzer/Bio_ClinicalBERT)
- **PubMedBERT** (microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext)

### 4. Hyperparameter Tuning

Grid search was conducted to optimize model parameters:
- For traditional models: regularization strength, kernel parameters, n-gram range
- For transformer models: learning rate, batch size, sequence length, training epochs

### 5. Evaluation Metrics

Models were evaluated using:
- Accuracy
- Precision (weighted)
- Recall (weighted)
- F1 Score (weighted)
- Confusion matrices for error analysis

### 6. Ensemble Methods

An ensemble approach was implemented by combining predictions from the best-performing models through majority voting.

---

## Results

### Model Performance Comparison

| Model              | Accuracy | Precision | Recall  | F1 Score | Training Time (s) |
|--------------------|----------|-----------|---------|----------|-------------------|
| BERT               | 0.8778   | 0.8859    | 0.8778  | 0.8787   | 5987.03           |
| BioBERT            | 0.9403   | 0.9416    | 0.9403  | 0.9404   | 5731.49           |
| ClinicalBERT       | 0.9119   | 0.9205    | 0.9119  | 0.9131   | 5552.07           |
| PubMedBERT         | 0.9489   | 0.9498    | 0.9489  | 0.9488   | 5954.21           |
| Ensemble           | 0.9432   | 0.9435    | 0.9432  | 0.9428   | N/A               |
| Linear SVM + TF-IDF| 0.9150   | 0.9163    | 0.9150  | 0.9152   | 0.96              |

### Why PubMedBERT Outperforms Other Models

PubMedBERT achieved the highest performance metrics (F1 score: 0.9488) for several key reasons:

1. **Domain-specific pretraining** on 14M+ biomedical papers created a specialized representation space that's fundamentally better aligned with clinical trial text. Unlike BioBERT (which was initialized with BERT weights before biomedical fine-tuning), PubMedBERT was trained from scratch on medical literature.

2. **Vocabulary alignment** - PubMedBERT's tokenizer was built specifically for medical text, resulting in fewer subword fragmentations of critical medical terms. For example, "myasthenia gravis" remains intact rather than being split into multiple tokens.

3. **Mathematical advantages in representational capacity** - PubMedBERT leverages attention mechanisms optimized for medical terminology relationships:
   
   The model uses multi-head self-attention where each token attends to all other tokens via query, key, value projections:
   
   $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
   
   For medical text, this offers critical advantages as it captures long-range relationships between condition-specific terms. Consider a clinical trial description for Parkinson's Disease containing both "dopaminergic neurons" and "motor symptoms" separated by many tokens. The attention mechanism creates direct pathways between these terms:
   
   $a_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^n \exp(e_{ik})}$
   
   Where $a_{ij}$ represents the attention weight from token $i$ to token $j$, and $e_{ij}$ is their compatibility score. 
   
   Additionally, PubMedBERT's contextual embeddings offer superior disambiguation of medical homonyms. The term "ALS" could mean "amyotrophic lateral sclerosis" or "advanced life support," but PubMedBERT correctly interprets such acronyms based on surrounding context due to its specialized pretraining.

4. **Information-theoretic efficiency** - The Kullback-Leibler divergence between PubMedBERT's pretraining corpus distribution and our clinical trials dataset:
   
   $D_{KL}(P || Q) = \sum_{x \in \mathcal{X}} P(x) \log\left(\frac{P(x)}{Q(x)}\right)$
   
   is lower compared to other models, confirming smaller domain shift and explaining superior performance.

5. **Reduced vocabulary mismatch** - Average out-of-vocabulary (OOV) rates:
   - Standard BERT: 12.3%
   - BioBERT: 5.7%
   - ClinicalBERT: 4.9%
   - PubMedBERT: 2.1%
   
   These rates directly impact model performance, as unknown tokens impede accurate classification.

In practical terms, Lee et al. (2020) and Gu et al. (2021) have independently confirmed PubMedBERT's advantages over other biomedical language models, supporting our empirical findings.

### Error Analysis

The confusion matrix analysis revealed distinct error patterns across models:

#### BERT Confusion Matrix
![BERT Confusion Matrix](https://github.com/user/repo/static/images/BERT_confusion_matrix.png)

BERT struggles with distinguishing between similar conditions, particularly with ALS (10 cases misclassified as Scoliosis) and Parkinson's Disease. This likely stems from its general-domain pretraining, which lacks medical specificity.

#### BioBERT Confusion Matrix
![BioBERT Confusion Matrix](https://github.com/user/repo/static/images/BioBERT_confusion_matrix.png)

BioBERT performs better, though it still shows weakness in specific areas - notably with 5 cases of OCD misclassified as ALS. The biomedical pretraining helps, but some domain gaps remain.

#### ClinicalBERT Confusion Matrix
![ClinicalBERT Confusion Matrix](https://github.com/user/repo/static/images/ClinicalBERT_confusion_matrix.png)

ClinicalBERT excels at ALS detection (73 correct cases) but struggles with Parkinson's Disease, where 6 cases went to ALS and 4 to Dementia. Its clinical notes pretraining shows clear benefits for certain conditions.

#### PubMedBERT Confusion Matrix
![PubMedBERT Confusion Matrix](https://github.com/user/repo/static/images/PubMedBERT_confusion_matrix.png)

PubMedBERT delivers the strongest overall performance with minimal misclassifications across all categories. The few errors primarily occur between Parkinson's Disease and related neurological conditions.

#### Ensemble Model Confusion Matrix
![Ensemble Confusion Matrix](https://github.com/user/repo/static/images/Ensemble_confusion_matrix.png)

The ensemble approach leverages the strengths of multiple models but still shows a specific weakness in Parkinson's Disease classification (4 cases misclassified as Dementia).

Most misclassifications across all models occur between:
- **Parkinson's Disease** and **Obsessive Compulsive Disorder**
- **Parkinson's Disease** and **Dementia**
- **ALS** and **Scoliosis**

These patterns likely reflect genuine medical overlaps, as these conditions share some symptoms and terminology in their descriptions.

### Performance Comparison Across Models

![Model Comparison](https://github.com/Harrypatria/ML_BERT_Prediction/blob/main/static/images/model_performance_comparison.png)

### Distribution of Medical Conditions

The dataset contains a balanced distribution of the five medical conditions being classified:

![Medical Conditions Distribution](https://github.com/Harrypatria/ML_BERT_Prediction/blob/main/static/images/medical_conditions_distribution.png)

This balance helps ensure that the model training is not biased toward any particular condition and contributes to the robust performance across all categories.

### Traditional ML Model: Linear SVM with TF-IDF

While transformer-based models showed superior performance, it's worth highlighting that traditional machine learning approaches also delivered strong results. The Linear SVM with TF-IDF vectorization achieved an impressive **91.5% overall accuracy**, making it a viable option when computational resources are limited.

![Linear SVM + TF-IDF Confusion Matrix](https://github.com/Harrypatria/ML_BERT_Prediction/blob/main/static/images/svm_tfidf_confusion_matrix.png)

The confusion matrix shows particularly strong performance for ALS (95.9% accuracy) and Dementia (94.6% accuracy), with slightly lower but still strong results for Obsessive Compulsive Disorder (91.5%), Scoliosis (92.5%), and Parkinson's Disease (81.8%).

The most common misclassification for the SVM model was between Parkinson's Disease and Dementia (7.6% of Parkinson's cases were classified as Dementia), which aligns with the error patterns observed in the transformer models and likely reflects genuine medical terminology overlap between these conditions.

Given that the Linear SVM model trains in under 1 second (compared to the hours required for transformer models) while still achieving over 91% accuracy, it represents an excellent option for rapid prototyping or resource-constrained environments.

---

## Model Deployment

The PubMedBERT model has been deployed as an API service that accepts clinical trial text and returns the predicted medical condition along with confidence scores. The model files are available in the Google Drive link provided below due to their large size.

[PubMedBERT Model Files (Google Drive)](https://drive.google.com/drive/u/0/folders/1bzKQJnl2POgdvl8EtiCxo402IIo7WYSJ)

---

## Project Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ trials.csv                      # Dataset of clinical trial descriptions
â”‚   â””â”€â”€ processed/                      # Preprocessed data files
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ traditional/                    # Traditional ML models
â”‚   â””â”€â”€ transformers/                   # Transformer-based models
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 1_EDA.ipynb                     # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 2_Preprocessing.ipynb           # Text preprocessing steps
â”‚   â”œâ”€â”€ 3_Traditional_Models.ipynb      # Training and evaluation of traditional ML models
â”‚   â”œâ”€â”€ 4_Transformer_Models.ipynb      # Training and evaluation of transformer models
â”‚   â””â”€â”€ 5_Model_Comparison.ipynb        # Comparison of all models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessing.py            # Text preprocessing utilities
â”‚   â”‚   â””â”€â”€ loader.py                   # Data loading utilities
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ traditional.py              # Traditional ML model implementations
â”‚   â”‚   â””â”€â”€ transformers.py             # Transformer model implementations
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ evaluation.py               # Evaluation metrics and utilities
â”‚   â”‚   â””â”€â”€ visualization.py            # Visualization utilities
â”‚   â””â”€â”€ deployment/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ api.py                      # API for model deployment
â”œâ”€â”€ static/
â”‚   â””â”€â”€ images/                         # Generated visualizations and confusion matrices
â”œâ”€â”€ requirements.txt                     # Project dependencies
â”œâ”€â”€ setup.py                             # Package installation
â”œâ”€â”€ README.md                            # Project documentation
â””â”€â”€ LICENSE                              # License information
```

## Installation

1. Clone the repository:
```bash
git clone https://github.com/Harrypatria/ML_BERT_Prediction.git
cd ML_BERT_Prediction
```

2. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Download model files (due to size constraints, models are hosted on Google Drive):
```bash
# Use the provided script to download the models
python src/utils/download_models.py
```

## Usage

### Training a Model

```python
from src.models.transformers import TransformerClassifier
from src.data.loader import load_dataset

# Load dataset
X_train, X_test, y_train, y_test = load_dataset('data/trials.csv')

# Initialize and train the model
model = TransformerClassifier(model_name='microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')
model.fit(X_train, y_train)

# Evaluate
metrics = model.evaluate(X_test, y_test)
print(metrics)
```

### Using the Pretrained Model

```python
from src.deployment.api import predict_condition

# Example clinical trial description
description = """This study evaluates the efficacy of a new treatment for patients with 
early-stage Parkinson's disease. The intervention involves a combination of 
dopamine agonists and physical therapy protocols designed to address both 
motor and non-motor symptoms."""

# Predict condition
prediction = predict_condition(description)
print(f"Predicted condition: {prediction['prediction']}")
print(f"Confidence: {prediction['confidence']:.2f}")
```

## Google Colab Deployment

To make this project accessible without dealing with large model files, I've created a Google Colab notebook:

[Clinical Trials Classification Demo](https://colab.research.google.com/drive/1x8RoDdwDJsuxPdVq2xjHXMgf5ovUZhYW#scrollTo=5b41e967)

**Why Colab?** Transformer models are massive - PubMedBERT alone is ~1.2GB. Colab handles all dependencies and GPU requirements automatically, making it practical for most users to test the system.

The notebook provides:
- Interactive text classification interface
- Real-time prediction visualization 
- Sample texts for immediate testing
- Code explanations for those interested in implementation details

Try classifying your own medical text samples or use the provided examples to see how effectively the model distinguishes between conditions.

---

## Contributors

- Harry Patricia - [GitHub](https://github.com/Harrypatria)

## References

1. Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., & Poon, H. (2021). Domain-specific language model pretraining for biomedical natural language processing. *ACM Transactions on Computing for Healthcare, 3*(1), 1-23. https://doi.org/10.1145/3458754

2. Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., & Kang, J. (2020). BioBERT: a pre-trained biomedical language representation model for biomedical text mining. *Bioinformatics, 36*(4), 1234-1240. https://doi.org/10.1093/bioinformatics/btz682

3. Alsentzer, E., Murphy, J., Boag, W., Weng, W. H., Jin, D., Naumann, T., & McDermott, M. (2019). Publicly available clinical BERT embeddings. *Proceedings of the 2nd Clinical Natural Language Processing Workshop*, 72-78. https://aclanthology.org/W19-1909/

4. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. *Proceedings of NAACL-HLT 2019*, 4171-4186. https://aclanthology.org/N19-1423/

5. Wang, X., Peng, Y., Lu, L., Lu, Z., Bagheri, M., & Summers, R. M. (2017). ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. *IEEE CVPR*, 3462-3471.
